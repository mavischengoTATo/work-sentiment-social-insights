{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec2ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweepy version = 4.12.1\n",
      "SEARCH-0 COMPLETED;  TWEETS_COLLECTED= 100 ; TIME (s) =  602.8242211341858\n",
      "SEARCH-10 COMPLETED;  TWEETS_COLLECTED= 1100 ; TIME (s) =  609.6143691539764\n",
      "SEARCH-20 COMPLETED;  TWEETS_COLLECTED= 2100 ; TIME (s) =  616.7975571155548\n",
      "SEARCH-30 COMPLETED;  TWEETS_COLLECTED= 3100 ; TIME (s) =  624.7018320560455\n",
      "SEARCH-40 COMPLETED;  TWEETS_COLLECTED= 4100 ; TIME (s) =  636.3923442363739\n",
      "SEARCH-50 COMPLETED;  TWEETS_COLLECTED= 5100 ; TIME (s) =  644.432225227356\n",
      "6000 60\n",
      "search time (s) = 655.1603240966797\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THE REDUNDANT IMPORTS AND FUNCTION DEFINITIONS IS INTENTIONAL TO MAKE THE CODE CELLS SELF CONTAINED \n",
    "import json\n",
    "from logging import raiseExceptions \n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# PRINT TWEEPY VERSION\n",
    "print(\"tweepy version =\",tweepy.__version__)\n",
    "\n",
    "#----------------------\n",
    "# READ API KEY FILE\n",
    "#----------------------\n",
    "f = open(\"/Users/xiaocheng/Downloads/api-keys.json\")\n",
    "input=json.load(f); #\n",
    "#print(input)\n",
    "\n",
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]   \n",
    "\n",
    "#----------------------\n",
    "# DEFINE USEFUL FUNCTIONS\n",
    "#----------------------\n",
    "\n",
    "# DEFINE PRETTY PRINT JSON FUNCTION\n",
    "def pretty_print_json(input):\n",
    "    print(json.dumps(input, indent=4))\n",
    "\n",
    "# DEFINE FUNCTION TO SAVE TWEEPY SEARCH RESULTS\n",
    "#   searches=array with various tweepy search objects\n",
    "#   TODO: ADD \"full and sparse\" mode\n",
    "#          full = save all tweet data (100 tweeks ~ 1 MB  --> 100,000 ~ 1 GB)\n",
    "#          sparse = only save most important info\n",
    "def save_search_tweets_results(searches,info_str=\"\",output_name=\"tweet-search.json\"):\n",
    "    # if(str(type(input)) == \"<class 'tweepy.models.SearchResults'>\"):\n",
    "    if(str(type(searches)) == \"<class 'list'>\"):\n",
    "        #COMBINE ALL JSONS FOR VARIOUS TWEETS INTO ON BIG JSON CALL \"out\"\n",
    "        out={}\n",
    "        out[\"search_info\"]=info_str\n",
    "\n",
    "        #LOOP OVER SEARCHES\n",
    "        tweet_ids=[]\n",
    "        k=0 #counter\n",
    "        for search in searches:\n",
    "            #LOOP OVER TWEETS IN SEARCH\n",
    "            for i in range(0,len(search)):\n",
    "                out[str(k)]=search[i]._json\n",
    "                tweet_id=search[i]._json[\"id_str\"]\n",
    "                #CHECK FOR REDUNDANT TWEETS\n",
    "                if tweet_id in tweet_ids:\n",
    "                    print(\"WARNING: REPEATED TWEETS IN SAVED FILE; ID = \",tweet_id)\n",
    "                tweet_ids.append(search[i]._json[\"id_str\"])\n",
    "\n",
    "                k+=1\n",
    "            #pretty_print_json(out)\n",
    "\n",
    "        #DELETE FILE IF IT EXIST (START FRESH)\n",
    "        if os.path.exists(output_name):\n",
    "            os.remove(output_name)\n",
    "\n",
    "        #WRITE FILE\n",
    "        with open(output_name, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    else: \n",
    "        raise RuntimeError(\"ERROR: Incorrect datatype\")\n",
    "\n",
    "#----------------------\n",
    "# SET UP CONNECTION\n",
    "#----------------------\n",
    "#   Use special options when initializing the API. These tell\n",
    "#   it to wait while the Twitter time-limit windows elapse\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "\n",
    "#----------------------\n",
    "# RUN SEARCH \n",
    "#----------------------\n",
    "\n",
    "#SEARCH PARAM\n",
    "query=\"new graduate\"\n",
    "\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "# number_of_tweets=1000 \n",
    "number_of_tweets=6000\n",
    "# ideally use multiples of 100 for number_of_tweets\n",
    "# should be able to collect 18000 tweets every 15 minutes\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "\n",
    "# THIS WILL KEEP DOING SEARCHES FURTHER AND FURTHER BACK IN TIME\n",
    "# USING THE MAX_ID TO THE TIMELINE \n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "#KEEP SEARCHING UNTIL DESIRED NUMBER OF TWEETS COLLECTED\n",
    "while num_tweets_collected<number_of_tweets or (time.time()-start_time)/60./60>max_loop_time_hrs: \n",
    "    try: \n",
    "        #FIRST SEARCH\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100)\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100,max_id=max_id_next)\n",
    "\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        #SAVE SEARCH RESULTS\n",
    "        searches.append(search_results)\n",
    "\n",
    "        #SAVE TEMPORARY CHECKPOINTS (DONT DO TOO OFTEN .. SLOWS CODE DOWN)\n",
    "        if(k%10==0):\n",
    "            print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "        if(k%25==0):\n",
    "            save_search_tweets_results(searches,output_name=\"tmp-snapshot.json\")\n",
    "            \n",
    "        k+=1\n",
    "    except:\n",
    "        print(\"WARNING: twitter search failed\")\n",
    "\n",
    "    #SLEEP 5 SECONDS BEFORE NEXT REQUEST \n",
    "    if(number_of_tweets>18000):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "\n",
    "# REPORT BASIC SEARCH INFO\n",
    "print(num_tweets_collected,len(searches))\n",
    "print(\"search time (s) =\", (time.time() - start_time)) #/60.)\n",
    "\n",
    "#TIMESTAMP SEARCH \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%Y-H%H-M%M-S%S\")\n",
    "\n",
    "#----------------------\n",
    "# SAVE RESULTS\n",
    "#----------------------\n",
    "info_str=\"query = \"+query+\"; number_of_tweets = \"+str(number_of_tweets)+\"; date = \"+str(dt_string)\n",
    "out_name=str(dt_string)+\"-twitter-search.json\"\n",
    "save_search_tweets_results(searches,info_str=info_str,output_name=out_name)\n",
    "\n",
    "#CLEAN-UP TEMP FILES\n",
    "os.remove(\"tmp-snapshot.json\")\n",
    "# import glob\n",
    "# list_to_delete=glob.glob(\"./*-tmp-snapshot.json\")\n",
    "# for file in list_to_delete:\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd796fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('2022-12-2022-H23-M52-S35-twitter-search.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data).T\n",
    "datadf = df.drop(\"search_info\")\n",
    "datadf.to_csv('raw_data1.csv')\n",
    "import pandas as pd\n",
    "r = pd.read_csv(\"raw_data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4231e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweepy version = 4.12.1\n",
      "SEARCH-0 COMPLETED;  TWEETS_COLLECTED= 86 ; TIME (s) =  0.4633622169494629\n",
      "SEARCH-10 COMPLETED;  TWEETS_COLLECTED= 1031 ; TIME (s) =  7.978933095932007\n",
      "SEARCH-20 COMPLETED;  TWEETS_COLLECTED= 1761 ; TIME (s) =  14.799990177154541\n",
      "SEARCH-30 COMPLETED;  TWEETS_COLLECTED= 2752 ; TIME (s) =  22.47602605819702\n",
      "SEARCH-40 COMPLETED;  TWEETS_COLLECTED= 3752 ; TIME (s) =  30.809023141860962\n",
      "SEARCH-50 COMPLETED;  TWEETS_COLLECTED= 4752 ; TIME (s) =  40.17376399040222\n",
      "SEARCH-60 COMPLETED;  TWEETS_COLLECTED= 5752 ; TIME (s) =  49.970667123794556\n",
      "6052 64\n",
      "search time (s) = 52.854995012283325\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THE REDUNDANT IMPORTS AND FUNCTION DEFINITIONS IS INTENTIONAL TO MAKE THE CODE CELLS SELF CONTAINED \n",
    "import json\n",
    "from logging import raiseExceptions \n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# PRINT TWEEPY VERSION\n",
    "print(\"tweepy version =\",tweepy.__version__)\n",
    "\n",
    "#----------------------\n",
    "# READ API KEY FILE\n",
    "#----------------------\n",
    "f = open(\"/Users/xiaocheng/Downloads/api-keys.json\")\n",
    "input=json.load(f); #\n",
    "#print(input)\n",
    "\n",
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]   \n",
    "\n",
    "#----------------------\n",
    "# DEFINE USEFUL FUNCTIONS\n",
    "#----------------------\n",
    "\n",
    "# DEFINE PRETTY PRINT JSON FUNCTION\n",
    "def pretty_print_json(input):\n",
    "    print(json.dumps(input, indent=4))\n",
    "\n",
    "# DEFINE FUNCTION TO SAVE TWEEPY SEARCH RESULTS\n",
    "#   searches=array with various tweepy search objects\n",
    "#   TODO: ADD \"full and sparse\" mode\n",
    "#          full = save all tweet data (100 tweeks ~ 1 MB  --> 100,000 ~ 1 GB)\n",
    "#          sparse = only save most important info\n",
    "def save_search_tweets_results(searches,info_str=\"\",output_name=\"tweet-search.json\"):\n",
    "    # if(str(type(input)) == \"<class 'tweepy.models.SearchResults'>\"):\n",
    "    if(str(type(searches)) == \"<class 'list'>\"):\n",
    "        #COMBINE ALL JSONS FOR VARIOUS TWEETS INTO ON BIG JSON CALL \"out\"\n",
    "        out={}\n",
    "        out[\"search_info\"]=info_str\n",
    "\n",
    "        #LOOP OVER SEARCHES\n",
    "        tweet_ids=[]\n",
    "        k=0 #counter\n",
    "        for search in searches:\n",
    "            #LOOP OVER TWEETS IN SEARCH\n",
    "            for i in range(0,len(search)):\n",
    "                out[str(k)]=search[i]._json\n",
    "                tweet_id=search[i]._json[\"id_str\"]\n",
    "                #CHECK FOR REDUNDANT TWEETS\n",
    "                if tweet_id in tweet_ids:\n",
    "                    print(\"WARNING: REPEATED TWEETS IN SAVED FILE; ID = \",tweet_id)\n",
    "                tweet_ids.append(search[i]._json[\"id_str\"])\n",
    "\n",
    "                k+=1\n",
    "            #pretty_print_json(out)\n",
    "\n",
    "        #DELETE FILE IF IT EXIST (START FRESH)\n",
    "        if os.path.exists(output_name):\n",
    "            os.remove(output_name)\n",
    "\n",
    "        #WRITE FILE\n",
    "        with open(output_name, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    else: \n",
    "        raise RuntimeError(\"ERROR: Incorrect datatype\")\n",
    "\n",
    "#----------------------\n",
    "# SET UP CONNECTION\n",
    "#----------------------\n",
    "#   Use special options when initializing the API. These tell\n",
    "#   it to wait while the Twitter time-limit windows elapse\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "\n",
    "#----------------------\n",
    "# RUN SEARCH \n",
    "#----------------------\n",
    "\n",
    "#SEARCH PARAM\n",
    "query=\"job hunting\"\n",
    "\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "# number_of_tweets=1000 \n",
    "number_of_tweets=6000\n",
    "# ideally use multiples of 100 for number_of_tweets\n",
    "# should be able to collect 18000 tweets every 15 minutes\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "\n",
    "# THIS WILL KEEP DOING SEARCHES FURTHER AND FURTHER BACK IN TIME\n",
    "# USING THE MAX_ID TO THE TIMELINE \n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "#KEEP SEARCHING UNTIL DESIRED NUMBER OF TWEETS COLLECTED\n",
    "while num_tweets_collected<number_of_tweets or (time.time()-start_time)/60./60>max_loop_time_hrs: \n",
    "    try: \n",
    "        #FIRST SEARCH\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100)\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100,max_id=max_id_next)\n",
    "\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        #SAVE SEARCH RESULTS\n",
    "        searches.append(search_results)\n",
    "\n",
    "        #SAVE TEMPORARY CHECKPOINTS (DONT DO TOO OFTEN .. SLOWS CODE DOWN)\n",
    "        if(k%10==0):\n",
    "            print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "        if(k%25==0):\n",
    "            save_search_tweets_results(searches,output_name=\"tmp-snapshot.json\")\n",
    "            \n",
    "        k+=1\n",
    "    except:\n",
    "        print(\"WARNING: twitter search failed\")\n",
    "\n",
    "    #SLEEP 5 SECONDS BEFORE NEXT REQUEST \n",
    "    if(number_of_tweets>18000):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "\n",
    "# REPORT BASIC SEARCH INFO\n",
    "print(num_tweets_collected,len(searches))\n",
    "print(\"search time (s) =\", (time.time() - start_time)) #/60.)\n",
    "\n",
    "#TIMESTAMP SEARCH \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%Y-H%H-M%M-S%S\")\n",
    "\n",
    "#----------------------\n",
    "# SAVE RESULTS\n",
    "#----------------------\n",
    "info_str=\"query = \"+query+\"; number_of_tweets = \"+str(number_of_tweets)+\"; date = \"+str(dt_string)\n",
    "out_name=str(dt_string)+\"-twitter-search.json\"\n",
    "save_search_tweets_results(searches,info_str=info_str,output_name=out_name)\n",
    "\n",
    "#CLEAN-UP TEMP FILES\n",
    "os.remove(\"tmp-snapshot.json\")\n",
    "# import glob\n",
    "# list_to_delete=glob.glob(\"./*-tmp-snapshot.json\")\n",
    "# for file in list_to_delete:\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0976ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('2022-12-2022-H11-M30-S28-twitter-search.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data).T\n",
    "datadf = df.drop(\"search_info\")\n",
    "datadf.to_csv('raw_data2.csv')\n",
    "import pandas as pd\n",
    "r = pd.read_csv(\"raw_data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0de3938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweepy version = 4.12.1\n",
      "SEARCH-0 COMPLETED;  TWEETS_COLLECTED= 100 ; TIME (s) =  0.9849190711975098\n",
      "SEARCH-10 COMPLETED;  TWEETS_COLLECTED= 1098 ; TIME (s) =  10.693579912185669\n",
      "SEARCH-20 COMPLETED;  TWEETS_COLLECTED= 2092 ; TIME (s) =  19.351260900497437\n",
      "SEARCH-30 COMPLETED;  TWEETS_COLLECTED= 3089 ; TIME (s) =  28.710843086242676\n",
      "SEARCH-40 COMPLETED;  TWEETS_COLLECTED= 4084 ; TIME (s) =  37.510963916778564\n",
      "SEARCH-50 COMPLETED;  TWEETS_COLLECTED= 5084 ; TIME (s) =  46.54157304763794\n",
      "SEARCH-60 COMPLETED;  TWEETS_COLLECTED= 6065 ; TIME (s) =  56.35607385635376\n",
      "SEARCH-70 COMPLETED;  TWEETS_COLLECTED= 7059 ; TIME (s) =  65.16458702087402\n",
      "SEARCH-80 COMPLETED;  TWEETS_COLLECTED= 8056 ; TIME (s) =  76.18343114852905\n",
      "SEARCH-90 COMPLETED;  TWEETS_COLLECTED= 9051 ; TIME (s) =  85.84900403022766\n",
      "SEARCH-100 COMPLETED;  TWEETS_COLLECTED= 10045 ; TIME (s) =  94.60447406768799\n",
      "10045 101\n",
      "search time (s) = 97.55709314346313\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THE REDUNDANT IMPORTS AND FUNCTION DEFINITIONS IS INTENTIONAL TO MAKE THE CODE CELLS SELF CONTAINED \n",
    "import json\n",
    "from logging import raiseExceptions \n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# PRINT TWEEPY VERSION\n",
    "print(\"tweepy version =\",tweepy.__version__)\n",
    "\n",
    "#----------------------\n",
    "# READ API KEY FILE\n",
    "#----------------------\n",
    "f = open(\"/Users/xiaocheng/Downloads/api-keys.json\")\n",
    "input=json.load(f); #\n",
    "#print(input)\n",
    "\n",
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]   \n",
    "\n",
    "#----------------------\n",
    "# DEFINE USEFUL FUNCTIONS\n",
    "#----------------------\n",
    "\n",
    "# DEFINE PRETTY PRINT JSON FUNCTION\n",
    "def pretty_print_json(input):\n",
    "    print(json.dumps(input, indent=4))\n",
    "\n",
    "# DEFINE FUNCTION TO SAVE TWEEPY SEARCH RESULTS\n",
    "#   searches=array with various tweepy search objects\n",
    "#   TODO: ADD \"full and sparse\" mode\n",
    "#          full = save all tweet data (100 tweeks ~ 1 MB  --> 100,000 ~ 1 GB)\n",
    "#          sparse = only save most important info\n",
    "def save_search_tweets_results(searches,info_str=\"\",output_name=\"tweet-search.json\"):\n",
    "    # if(str(type(input)) == \"<class 'tweepy.models.SearchResults'>\"):\n",
    "    if(str(type(searches)) == \"<class 'list'>\"):\n",
    "        #COMBINE ALL JSONS FOR VARIOUS TWEETS INTO ON BIG JSON CALL \"out\"\n",
    "        out={}\n",
    "        out[\"search_info\"]=info_str\n",
    "\n",
    "        #LOOP OVER SEARCHES\n",
    "        tweet_ids=[]\n",
    "        k=0 #counter\n",
    "        for search in searches:\n",
    "            #LOOP OVER TWEETS IN SEARCH\n",
    "            for i in range(0,len(search)):\n",
    "                out[str(k)]=search[i]._json\n",
    "                tweet_id=search[i]._json[\"id_str\"]\n",
    "                #CHECK FOR REDUNDANT TWEETS\n",
    "                if tweet_id in tweet_ids:\n",
    "                    print(\"WARNING: REPEATED TWEETS IN SAVED FILE; ID = \",tweet_id)\n",
    "                tweet_ids.append(search[i]._json[\"id_str\"])\n",
    "\n",
    "                k+=1\n",
    "            #pretty_print_json(out)\n",
    "\n",
    "        #DELETE FILE IF IT EXIST (START FRESH)\n",
    "        if os.path.exists(output_name):\n",
    "            os.remove(output_name)\n",
    "\n",
    "        #WRITE FILE\n",
    "        with open(output_name, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    else: \n",
    "        raise RuntimeError(\"ERROR: Incorrect datatype\")\n",
    "\n",
    "#----------------------\n",
    "# SET UP CONNECTION\n",
    "#----------------------\n",
    "#   Use special options when initializing the API. These tell\n",
    "#   it to wait while the Twitter time-limit windows elapse\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "\n",
    "#----------------------\n",
    "# RUN SEARCH \n",
    "#----------------------\n",
    "\n",
    "#SEARCH PARAM\n",
    "query=\"job OR work\"\n",
    "\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "# number_of_tweets=1000 \n",
    "number_of_tweets=10000\n",
    "# ideally use multiples of 100 for number_of_tweets\n",
    "# should be able to collect 18000 tweets every 15 minutes\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "\n",
    "# THIS WILL KEEP DOING SEARCHES FURTHER AND FURTHER BACK IN TIME\n",
    "# USING THE MAX_ID TO THE TIMELINE \n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "#KEEP SEARCHING UNTIL DESIRED NUMBER OF TWEETS COLLECTED\n",
    "while num_tweets_collected<number_of_tweets or (time.time()-start_time)/60./60>max_loop_time_hrs: \n",
    "    try: \n",
    "        #FIRST SEARCH\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100)\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100,max_id=max_id_next)\n",
    "\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        #SAVE SEARCH RESULTS\n",
    "        searches.append(search_results)\n",
    "\n",
    "        #SAVE TEMPORARY CHECKPOINTS (DONT DO TOO OFTEN .. SLOWS CODE DOWN)\n",
    "        if(k%10==0):\n",
    "            print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "        if(k%25==0):\n",
    "            save_search_tweets_results(searches,output_name=\"tmp-snapshot.json\")\n",
    "            \n",
    "        k+=1\n",
    "    except:\n",
    "        print(\"WARNING: twitter search failed\")\n",
    "\n",
    "    #SLEEP 5 SECONDS BEFORE NEXT REQUEST \n",
    "    if(number_of_tweets>18000):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "\n",
    "# REPORT BASIC SEARCH INFO\n",
    "print(num_tweets_collected,len(searches))\n",
    "print(\"search time (s) =\", (time.time() - start_time)) #/60.)\n",
    "\n",
    "#TIMESTAMP SEARCH \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%Y-H%H-M%M-S%S\")\n",
    "\n",
    "#----------------------\n",
    "# SAVE RESULTS\n",
    "#----------------------\n",
    "info_str=\"query = \"+query+\"; number_of_tweets = \"+str(number_of_tweets)+\"; date = \"+str(dt_string)\n",
    "out_name=str(dt_string)+\"-twitter-search.json\"\n",
    "save_search_tweets_results(searches,info_str=info_str,output_name=out_name)\n",
    "\n",
    "#CLEAN-UP TEMP FILES\n",
    "os.remove(\"tmp-snapshot.json\")\n",
    "# import glob\n",
    "# list_to_delete=glob.glob(\"./*-tmp-snapshot.json\")\n",
    "# for file in list_to_delete:\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f2781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('2022-12-2022-H11-M34-S23-twitter-search.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data).T\n",
    "datadf = df.drop(\"search_info\")\n",
    "datadf.to_csv('raw_data3.csv')\n",
    "import pandas as pd\n",
    "r = pd.read_csv(\"raw_data3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db8a7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweepy version = 4.12.1\n",
      "SEARCH-0 COMPLETED;  TWEETS_COLLECTED= 99 ; TIME (s) =  0.6936612129211426\n",
      "SEARCH-10 COMPLETED;  TWEETS_COLLECTED= 1041 ; TIME (s) =  8.061005115509033\n",
      "SEARCH-20 COMPLETED;  TWEETS_COLLECTED= 2032 ; TIME (s) =  15.304491996765137\n",
      "SEARCH-30 COMPLETED;  TWEETS_COLLECTED= 3032 ; TIME (s) =  23.20733904838562\n",
      "SEARCH-40 COMPLETED;  TWEETS_COLLECTED= 4031 ; TIME (s) =  30.49046230316162\n",
      "SEARCH-50 COMPLETED;  TWEETS_COLLECTED= 5031 ; TIME (s) =  38.617154121398926\n",
      "SEARCH-60 COMPLETED;  TWEETS_COLLECTED= 6026 ; TIME (s) =  47.20142889022827\n",
      "6026 61\n",
      "search time (s) = 47.4037811756134\n"
     ]
    }
   ],
   "source": [
    "# NOTE: THE REDUNDANT IMPORTS AND FUNCTION DEFINITIONS IS INTENTIONAL TO MAKE THE CODE CELLS SELF CONTAINED \n",
    "import json\n",
    "from logging import raiseExceptions \n",
    "import tweepy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# PRINT TWEEPY VERSION\n",
    "print(\"tweepy version =\",tweepy.__version__)\n",
    "\n",
    "#----------------------\n",
    "# READ API KEY FILE\n",
    "#----------------------\n",
    "f = open(\"/Users/xiaocheng/Downloads/api-keys.json\")\n",
    "input=json.load(f); #\n",
    "#print(input)\n",
    "\n",
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]   \n",
    "\n",
    "#----------------------\n",
    "# DEFINE USEFUL FUNCTIONS\n",
    "#----------------------\n",
    "\n",
    "# DEFINE PRETTY PRINT JSON FUNCTION\n",
    "def pretty_print_json(input):\n",
    "    print(json.dumps(input, indent=4))\n",
    "\n",
    "# DEFINE FUNCTION TO SAVE TWEEPY SEARCH RESULTS\n",
    "#   searches=array with various tweepy search objects\n",
    "#   TODO: ADD \"full and sparse\" mode\n",
    "#          full = save all tweet data (100 tweeks ~ 1 MB  --> 100,000 ~ 1 GB)\n",
    "#          sparse = only save most important info\n",
    "def save_search_tweets_results(searches,info_str=\"\",output_name=\"tweet-search.json\"):\n",
    "    # if(str(type(input)) == \"<class 'tweepy.models.SearchResults'>\"):\n",
    "    if(str(type(searches)) == \"<class 'list'>\"):\n",
    "        #COMBINE ALL JSONS FOR VARIOUS TWEETS INTO ON BIG JSON CALL \"out\"\n",
    "        out={}\n",
    "        out[\"search_info\"]=info_str\n",
    "\n",
    "        #LOOP OVER SEARCHES\n",
    "        tweet_ids=[]\n",
    "        k=0 #counter\n",
    "        for search in searches:\n",
    "            #LOOP OVER TWEETS IN SEARCH\n",
    "            for i in range(0,len(search)):\n",
    "                out[str(k)]=search[i]._json\n",
    "                tweet_id=search[i]._json[\"id_str\"]\n",
    "                #CHECK FOR REDUNDANT TWEETS\n",
    "                if tweet_id in tweet_ids:\n",
    "                    print(\"WARNING: REPEATED TWEETS IN SAVED FILE; ID = \",tweet_id)\n",
    "                tweet_ids.append(search[i]._json[\"id_str\"])\n",
    "\n",
    "                k+=1\n",
    "            #pretty_print_json(out)\n",
    "\n",
    "        #DELETE FILE IF IT EXIST (START FRESH)\n",
    "        if os.path.exists(output_name):\n",
    "            os.remove(output_name)\n",
    "\n",
    "        #WRITE FILE\n",
    "        with open(output_name, 'w') as f:\n",
    "            json.dump(out, f)\n",
    "    else: \n",
    "        raise RuntimeError(\"ERROR: Incorrect datatype\")\n",
    "\n",
    "#----------------------\n",
    "# SET UP CONNECTION\n",
    "#----------------------\n",
    "#   Use special options when initializing the API. These tell\n",
    "#   it to wait while the Twitter time-limit windows elapse\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True) \n",
    "\n",
    "#----------------------\n",
    "# RUN SEARCH \n",
    "#----------------------\n",
    "\n",
    "#SEARCH PARAM\n",
    "query=\"retire\"\n",
    "\n",
    "# NUMBER OF TWEETS TO SEARCH \n",
    "# number_of_tweets=1000 \n",
    "number_of_tweets=6000\n",
    "# ideally use multiples of 100 for number_of_tweets\n",
    "# should be able to collect 18000 tweets every 15 minutes\n",
    "start_time = time.time()\n",
    "max_loop_time_hrs=5\n",
    "\n",
    "# THIS WILL KEEP DOING SEARCHES FURTHER AND FURTHER BACK IN TIME\n",
    "# USING THE MAX_ID TO THE TIMELINE \n",
    "num_tweets_collected=0\n",
    "searches=[]\n",
    "k=0\n",
    "#KEEP SEARCHING UNTIL DESIRED NUMBER OF TWEETS COLLECTED\n",
    "while num_tweets_collected<number_of_tweets or (time.time()-start_time)/60./60>max_loop_time_hrs: \n",
    "    try: \n",
    "        #FIRST SEARCH\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100)\n",
    "        #ADDITIONAL SEARCHES\n",
    "        else:\n",
    "            search_results = api.search_tweets(query, lang=\"en\", count=100,max_id=max_id_next)\n",
    "\n",
    "        #UPDATE PARAMETERS\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        #SAVE SEARCH RESULTS\n",
    "        searches.append(search_results)\n",
    "\n",
    "        #SAVE TEMPORARY CHECKPOINTS (DONT DO TOO OFTEN .. SLOWS CODE DOWN)\n",
    "        if(k%10==0):\n",
    "            print(\"SEARCH-\"+str(k)+\" COMPLETED;  TWEETS_COLLECTED=\",num_tweets_collected,\"; TIME (s) = \",time.time() - start_time)\n",
    "        if(k%25==0):\n",
    "            save_search_tweets_results(searches,output_name=\"tmp-snapshot.json\")\n",
    "            \n",
    "        k+=1\n",
    "    except:\n",
    "        print(\"WARNING: twitter search failed\")\n",
    "\n",
    "    #SLEEP 5 SECONDS BEFORE NEXT REQUEST \n",
    "    if(number_of_tweets>18000):\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "\n",
    "# REPORT BASIC SEARCH INFO\n",
    "print(num_tweets_collected,len(searches))\n",
    "print(\"search time (s) =\", (time.time() - start_time)) #/60.)\n",
    "\n",
    "#TIMESTAMP SEARCH \n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%Y-H%H-M%M-S%S\")\n",
    "\n",
    "#----------------------\n",
    "# SAVE RESULTS\n",
    "#----------------------\n",
    "info_str=\"query = \"+query+\"; number_of_tweets = \"+str(number_of_tweets)+\"; date = \"+str(dt_string)\n",
    "out_name=str(dt_string)+\"-twitter-search.json\"\n",
    "save_search_tweets_results(searches,info_str=info_str,output_name=out_name)\n",
    "\n",
    "#CLEAN-UP TEMP FILES\n",
    "os.remove(\"tmp-snapshot.json\")\n",
    "# import glob\n",
    "# list_to_delete=glob.glob(\"./*-tmp-snapshot.json\")\n",
    "# for file in list_to_delete:\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7ada9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('2022-12-2022-H11-M47-S41-twitter-search.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data).T\n",
    "datadf = df.drop(\"search_info\")\n",
    "datadf.to_csv('raw_data4.csv')\n",
    "import pandas as pd\n",
    "r = pd.read_csv(\"raw_data4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34d126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
